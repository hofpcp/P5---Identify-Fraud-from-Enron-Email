{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import all necessary libraries for manipulating data and building the ML models \n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". Added is one new \"bonus_salary_multiple\"\n",
    "\n",
    "features_list = ['poi','salary', 'deferral_payments', 'total_payments', \n",
    "                 'loan_advances', 'bonus', 'restricted_stock_deferred', \n",
    "                 'deferred_income', 'total_stock_value', 'expenses', \n",
    "                 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                 'restricted_stock', 'director_fees','to_messages', \n",
    "                  'from_poi_to_this_person', 'from_messages',\n",
    "                 'from_this_person_to_poi', 'shared_receipt_with_poi', 'bonus_salary_multiple']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of records in dataset:\n",
      "146\n",
      "Total Number of columns in dataset:\n",
      "21\n",
      "Total Number of Poi's in dataset:\n",
      "18\n",
      "Total Number of NaN entries in dataset:\n",
      "1323\n"
     ]
    }
   ],
   "source": [
    "### Investigate dataset and count number of NaN entries\n",
    "\n",
    "print 'Total Number of records in dataset:' \n",
    "print len(data_dict)\n",
    "\n",
    "print 'Total Number of columns in dataset:' \n",
    "print len(data_dict['METTS MARK'])\n",
    "\n",
    "\n",
    "count = 0\n",
    "for employee in data_dict:\n",
    "    if data_dict[employee]['poi'] == True:\n",
    "        count += 1\n",
    "print 'Total Number of Poi\\'s in dataset:'\n",
    "print count\n",
    "\n",
    "nan_features = features_list\n",
    "nan_features.remove('bonus_salary_multiple')\n",
    "nan_count = 0\n",
    "for employee in data_dict:\n",
    "    for feature in nan_features:\n",
    "        if data_dict[employee][feature] == 'NaN':\n",
    "         nan_count += 1\n",
    "\n",
    "print 'Total Number of NaN entries in dataset:'\n",
    "print nan_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 2: Remove outliers. From viewing the relationship between Salary and Bonus \"Total\" identified and removed.\n",
    "### Visual inspection of the dataset furthermore revealed a non-person \"The Travel Agency in the Park\" also removed.\n",
    "\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "data_dict.pop('The Travel Agency In the Park',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s) New feature \"bonus_salary_multiple\" added to dataset and feature list.\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "\n",
    "for employee in data_dict:\n",
    "    if data_dict[employee]['salary'] > 0 and data_dict[employee]['bonus'] > 0:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = round(float(data_dict[employee]['bonus'])\n",
    "                                                             /float(data_dict[employee]['salary']),2)\n",
    "    else:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = float(0.0)\n",
    "        \n",
    "for employee in data_dict:\n",
    "    for i in features_list:\n",
    "        if data_dict[employee][i] == 'NaN' or data_dict[employee][i] == 'nan':\n",
    "            data_dict[employee][i] = 0.0\n",
    "\n",
    "\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by SelectKBest:\n",
      "[('exercised_stock_options', 25.097541528735491), ('total_stock_value', 24.467654047526398), ('bonus', 21.060001707536571), ('salary', 18.575703268041785), ('deferred_income', 11.595547659730601), ('long_term_incentive', 10.072454529369441), ('restricted_stock', 9.3467007910514877), ('total_payments', 8.8667215371077717), ('shared_receipt_with_poi', 8.7464855321290802), ('loan_advances', 7.2427303965360181), ('expenses', 6.2342011405067401), ('from_poi_to_this_person', 5.3449415231473374), ('other', 4.204970858301416), ('from_this_person_to_poi', 2.4265081272428781), ('director_fees', 2.1076559432760908), ('to_messages', 1.6988243485808501), ('deferral_payments', 0.2170589303395084), ('from_messages', 0.16416449823428736), ('restricted_stock_deferred', 0.06498431172371151)]\n"
     ]
    }
   ],
   "source": [
    "### Features reduced via SelectKBest printed including scoring per feature\n",
    "### For reasons I have not been able to identify the new Feature was needed to be run twice (therefore duplicated here)\n",
    "### In order for SelectKBest to run successfully\n",
    "\n",
    "for employee in data_dict:\n",
    "    if data_dict[employee]['salary'] > 0 and data_dict[employee]['bonus'] > 0:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = round(float(data_dict[employee]['bonus'])\n",
    "                                                             /float(data_dict[employee]['salary']),2)\n",
    "    else:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = float(0.0)\n",
    "\n",
    "\n",
    "for employee in data_dict:\n",
    "    for i in features_list:\n",
    "        if data_dict[employee][i] == 'NaN' or data_dict[employee][i] == 'nan':\n",
    "            data_dict[employee][i] = 0.0\n",
    "\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "kbest = SelectKBest()\n",
    "kbest = SelectKBest(k='all')\n",
    "selected_features = kbest.fit_transform(features,labels)\n",
    "\n",
    "\n",
    "features_selected=[features_list[i+1] for i in kbest.get_support(indices=True)]\n",
    "print 'Features selected by SelectKBest:'\n",
    "### print features_selected\n",
    "\n",
    "### print kbest.scores_\n",
    "\n",
    "kbest_lst = []\n",
    "for i in kbest.scores_:\n",
    "    kbest_lst.append(i)\n",
    "\n",
    "kbest_dict = dict(zip(features_selected, kbest_lst))\n",
    "\n",
    "kbest_lst = sorted(kbest_lst, reverse=True)\n",
    "\n",
    "import operator\n",
    "sorted_kbest_dict = sorted(kbest_dict.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "matplotlib.pyplot.plot(kbest_lst)\n",
    "matplotlib.pyplot.xlabel(\"Feature\")\n",
    "matplotlib.pyplot.ylabel(\"kScore\")\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "\n",
    "print sorted_kbest_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by SelectKBest:\n",
      "['salary', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive']\n",
      "['poi', 'salary', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive']\n"
     ]
    }
   ],
   "source": [
    "### From analysis of kBest the 4 highest scoring features are chosen going forward\n",
    "### Features list updated with the SelectKBest and 'poi' added as first element\n",
    "\n",
    "kbest = SelectKBest(k=6)\n",
    "selected_features = kbest.fit_transform(features,labels)\n",
    "\n",
    "\n",
    "features_selected=[features_list[i+1] for i in kbest.get_support(indices=True)]\n",
    "print 'Features selected by SelectKBest:'\n",
    "print features_selected\n",
    "\n",
    "\n",
    "\n",
    "poi = 'poi'\n",
    "features_list = [poi] + features_selected\n",
    "print features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "### data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "### labels, features = targetFeatureSplit(data)\n",
    "### features scaled to between 0-1 via the MinMaxScaler\n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "### For the initial test of chosen ML Algorithms features/labels split into train/test sets\n",
    "\n",
    "labels_train, labels_test, features_train, features_test = train_test_split(labels, features, test_size = 0.3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Validation\n",
      "Score:  0.928571428571\n",
      "Presicion:  0.5\n",
      "Recall:  0.666666666667 \n",
      "\n",
      "SVM Validation\n",
      "Score:  0.928571428571\n",
      "Presicion:  0.0\n",
      "Recall:  0.0 \n",
      "\n",
      "RandomForrest Validation\n",
      "Score:  0.904761904762\n",
      "Presicion:  0.428571428571\n",
      "Recall:  1.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# GaussianNB classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "gaus_sc = clf.score(features_test, labels_test)\n",
    "\n",
    "print 'GaussianNB Validation'\n",
    "print \"Score: \", gaus_sc\n",
    "pre_gaus = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Presicion: \",pre_gaus\n",
    "rec_gaus = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_gaus, '\\n'\n",
    "\n",
    "\n",
    "# Support Vector Machine classifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "svm_sc = clf.score(features_test, labels_test)\n",
    "\n",
    "print 'SVM Validation'\n",
    "print \"Score: \", svm_sc\n",
    "pre_svm = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Presicion: \",pre_svm\n",
    "rec_svm = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_svm, '\\n'\n",
    "\n",
    "\n",
    "# RandomForest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state = 0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "rf_sc = clf.score(features_test, labels_test)\n",
    "\n",
    "print 'RandomForrest Validation'\n",
    "print \"Score: \", rf_sc\n",
    "pre_rf = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Presicion: \",pre_rf\n",
    "rec_rf = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_rf, '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Further libraries loaded to do Pipeline, FeatureUnion, Gridsearch, PCA and StratifiedShufflesplit\n",
    "\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Validation of the \"Tuned\" Classifiers done via a StratifiedShuffleSplit  \n",
    "ss_split = StratifiedShuffleSplit(labels, 100, test_size = 0.5, random_state = 42)\n",
    "\n",
    "for train_idx, test_idx in ss_split: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Validation with Parameter Tuning\n",
      "Best Score: 0.877428571429\n",
      "Best parameters: {'max_features': 1}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=1, max_leaf_nodes=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "Precision:  0.8\n",
      "Recall:  0.444444444444\n"
     ]
    }
   ],
   "source": [
    "### RandomForest with Parameter Tuning\n",
    "\n",
    "clf = RandomForestClassifier(criterion = 'gini', min_samples_split = 2, max_leaf_nodes = None, max_depth = 5, \n",
    "                             min_samples_leaf = 2, random_state = 0, n_estimators = 200)\n",
    "\n",
    "parameters = {'max_features': [1,2,3,4]}\n",
    "\n",
    "grid_search_rf = GridSearchCV(clf, parameters, cv = ss_split)\n",
    "grid_search_rf.fit(features, labels)\n",
    "\n",
    "clf = grid_search_rf.best_estimator_\n",
    "\n",
    "clf.fit(features, labels)\n",
    "\n",
    "print 'RandomForest Validation with Parameter Tuning'\n",
    "print ('Best Score: {}'.format(grid_search_rf.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search_rf.best_params_))\n",
    "print clf\n",
    "\n",
    "pred_rf= clf.predict(features_test)\n",
    "\n",
    "pre_rf = precision_score(labels_test, pred_rf, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Precision: \",pre_rf\n",
    "rec_rf = recall_score(labels_test, pred_rf, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy SVC 0.9\n",
      "pre 1.0\n",
      "rec 0.222222222222\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', PCA(copy=True, n_components=6, whiten=False)), ('SupportVectorClassifier', SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
      "  gamma=0.001, kernel='linear', max_iter=-1, probability=False,\n",
      "  random_state=42, shrinking=True, tol=0.001, verbose=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Support Vector Machine with Parameter Tuning\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pipeline = Pipeline([('scaling',scaler),('reduce_dim', PCA()), (\"SupportVectorClassifier\", SVC(random_state=42))])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "          'SupportVectorClassifier__C': [1, 10, 100, 1000], \n",
    "          'SupportVectorClassifier__gamma': [0.001, 0.0001], \n",
    "          'SupportVectorClassifier__kernel': ['rbf', 'linear'],\n",
    "    'reduce_dim__n_components':[1, 2, 3, 4, 5, 6]\n",
    "}\n",
    "\n",
    "\n",
    "# pa = dict(reduce_dim__n_components=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid = param_grid, scoring = 'f1', cv = ss_split)\n",
    "gs.fit(features, labels)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "clf.fit(features, labels)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "accuracy = clf.score(features_test, labels_test)\n",
    "print \"accuracy SVC\",accuracy\n",
    "\n",
    "pre = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"pre\",pre\n",
    "rec = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"rec\",rec\n",
    "\n",
    "print clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy GaussianNB 0.871428571429\n",
      "pre 0.5\n",
      "rec 0.444444444444\n",
      "Pipeline(steps=[('scaling', MinMaxScaler(copy=True, feature_range=(0, 1))), ('reduce_dim', PCA(copy=True, n_components=3, whiten=False)), ('NaiveBayes', GaussianNB())])\n"
     ]
    }
   ],
   "source": [
    "### GaussianNB with Parameter Tuning\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pipeline = Pipeline([('scaling',scaler),('reduce_dim', PCA()),(\"NaiveBayes\", GaussianNB())])\n",
    "\n",
    "pa = dict(reduce_dim__n_components=[1, 2, 3, 4, 5, 6])\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid = pa, scoring = 'f1', cv = ss_split)\n",
    "gs.fit(features, labels)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "clf.fit(features, labels)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "accuracy = clf.score(features_test, labels_test)\n",
    "print \"accuracy GaussianNB\",accuracy\n",
    "\n",
    "pre = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"pre\",pre\n",
    "rec = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"rec\",rec\n",
    "\n",
    "print clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
