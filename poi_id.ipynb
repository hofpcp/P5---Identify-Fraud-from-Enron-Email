{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Import all necessary libraries for manipulating data and building the ML models \n",
    "\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"../tools/\")\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import matplotlib.pyplot\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\". Added is one new \"bonus_salary_multiple\"\n",
    "\n",
    "features_list = ['poi','salary', 'deferral_payments', 'total_payments', \n",
    "                 'loan_advances', 'bonus', 'restricted_stock_deferred', \n",
    "                 'deferred_income', 'total_stock_value', 'expenses', \n",
    "                 'exercised_stock_options', 'other', 'long_term_incentive', \n",
    "                 'restricted_stock', 'director_fees','to_messages', \n",
    "                  'from_poi_to_this_person', 'from_messages',\n",
    "                 'from_this_person_to_poi', 'shared_receipt_with_poi', 'bonus_salary_multiple']\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Task 2: Remove outliers. From viewing the relationship between Salary and Bonus \"Total\" identified and removed.\n",
    "### Visual inspection of the dataset furthermore revealed a non-person \"The Travel Agency in the Park\" also removed.\n",
    "\n",
    "features = [\"salary\", \"bonus\"]\n",
    "data = featureFormat(data_dict, features)\n",
    "for point in data:\n",
    "    salary = point[0]\n",
    "    bonus = point[1]\n",
    "    matplotlib.pyplot.scatter( salary, bonus )\n",
    "matplotlib.pyplot.xlabel(\"salary\")\n",
    "matplotlib.pyplot.ylabel(\"bonus\")\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "data_dict.pop('TOTAL',0)\n",
    "data_dict.pop('The Travel Agency In the Park',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Task 3: Create new feature(s) New feature \"bonus_salary_multiple\" added to dataset and feature list.\n",
    "### Store to my_dataset for easy export below.\n",
    "\n",
    "\n",
    "for employee in data_dict:\n",
    "    if data_dict[employee]['salary'] > 0 and data_dict[employee]['bonus'] > 0:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = round(float(data_dict[employee]['bonus'])\n",
    "                                                             /float(data_dict[employee]['salary']),2)\n",
    "    else:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = float(0.0)\n",
    "        \n",
    "for employee in data_dict:\n",
    "    for i in features_list:\n",
    "        if data_dict[employee][i] == 'NaN' or data_dict[employee][i] == 'nan':\n",
    "            data_dict[employee][i] = 0.0\n",
    "\n",
    "\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features selected by SelectKBest:\n",
      "['salary', 'total_payments', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'shared_receipt_with_poi', 'bonus_salary_multiple']\n",
      "[ 18.57570327   0.21705893   8.86672154   7.2427304   21.06000171\n",
      "   0.06498431  11.59554766  24.46765405   6.23420114  25.09754153\n",
      "   4.20497086  10.07245453   9.34670079   2.10765594   1.69882435\n",
      "   5.34494152   0.1641645    2.42650813   8.74648553  10.9576516 ]\n"
     ]
    }
   ],
   "source": [
    "### Features reduced via SelectKBest printed including scoring per feature\n",
    "### For reasons I have not been able to identify the new Feature was needed to be run twice (therefore duplicated here)\n",
    "### In order for SelectKBest to run successfully\n",
    "\n",
    "for employee in data_dict:\n",
    "    if data_dict[employee]['salary'] > 0 and data_dict[employee]['bonus'] > 0:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = round(float(data_dict[employee]['bonus'])\n",
    "                                                             /float(data_dict[employee]['salary']),2)\n",
    "    else:\n",
    "        data_dict[employee]['bonus_salary_multiple'] = float(0.0)\n",
    "\n",
    "\n",
    "for employee in data_dict:\n",
    "    for i in features_list:\n",
    "        if data_dict[employee][i] == 'NaN' or data_dict[employee][i] == 'nan':\n",
    "            data_dict[employee][i] = 0.0\n",
    "\n",
    "\n",
    "my_dataset = data_dict\n",
    "\n",
    "\n",
    "data = featureFormat(data_dict, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "kbest = SelectKBest()\n",
    "kbest = SelectKBest(k=10)\n",
    "selected_features = kbest.fit_transform(features,labels)\n",
    "\n",
    "\n",
    "features_selected=[features_list[i+1] for i in kbest.get_support(indices=True)]\n",
    "print 'Features selected by SelectKBest:'\n",
    "print features_selected\n",
    "\n",
    "print kbest.scores_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['poi', 'salary', 'total_payments', 'bonus', 'deferred_income', 'total_stock_value', 'exercised_stock_options', 'long_term_incentive', 'restricted_stock', 'shared_receipt_with_poi', 'bonus_salary_multiple']\n"
     ]
    }
   ],
   "source": [
    "### Features list updated with the SelectKBest and 'poi' added as first element\n",
    "\n",
    "poi = 'poi'\n",
    "features_list = [poi] + features_selected\n",
    "print features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "### data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "### labels, features = targetFeatureSplit(data)\n",
    "### features scaled to between 0-1 via the MinMaxScaler\n",
    "\n",
    "\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)\n",
    "\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "\n",
    "\n",
    "### For the initial test of chosen ML Algorithms features/labels split into train/test sets\n",
    "\n",
    "labels_train, labels_test, features_train, features_test = train_test_split(labels, features, test_size = 0.3, random_state = 42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB Validation\n",
      "Score:  0.886363636364\n",
      "Presicion:  0.5\n",
      "Recall:  0.6 \n",
      "\n",
      "SVM Validation\n",
      "Score:  0.886363636364\n",
      "Presicion:  0.0\n",
      "Recall:  0.0 \n",
      "\n",
      "RandomForrest Validation\n",
      "Score:  0.840909090909\n",
      "Presicion:  0.0\n",
      "Recall:  0.0 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:958: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Task 4: Try a varity of classifiers\n",
    "### Please name your classifier clf for easy export below.\n",
    "### Note that if you want to do PCA or other multi-stage operations,\n",
    "### you'll need to use Pipelines. For more info:\n",
    "### http://scikit-learn.org/stable/modules/pipeline.html\n",
    "\n",
    "# GaussianNB classifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "gaus_sc = clf.score(features_test, labels_test)\n",
    "\n",
    "print 'GaussianNB Validation'\n",
    "print \"Score: \", gaus_sc\n",
    "pre_gaus = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Presicion: \",pre_gaus\n",
    "rec_gaus = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_gaus, '\\n'\n",
    "\n",
    "\n",
    "# Support Vector Machine classifier\n",
    "from sklearn.svm import SVC\n",
    "clf = SVC()\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "svm_sc = clf.score(features_test, labels_test)\n",
    "\n",
    "print 'SVM Validation'\n",
    "print \"Score: \", svm_sc\n",
    "pre_svm = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Presicion: \",pre_svm\n",
    "rec_svm = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_svm, '\\n'\n",
    "\n",
    "\n",
    "# RandomForest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state = 0)\n",
    "clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)\n",
    "rf_sc = clf.score(features_test, labels_test)\n",
    "\n",
    "print 'RandomForrest Validation'\n",
    "print \"Score: \", rf_sc\n",
    "pre_rf = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Presicion: \",pre_rf\n",
    "rec_rf = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_rf, '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Task 5: Tune your classifier to achieve better than .3 precision and recall \n",
    "### using our testing script. Check the tester.py script in the final project\n",
    "### folder for details on the evaluation method, especially the test_classifier\n",
    "### function. Because of the small size of the dataset, the script uses\n",
    "### stratified shuffle split cross validation. For more info: \n",
    "### http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html\n",
    "\n",
    "# Further libraries loaded to do Pipeline, FeatureUnion, Gridsearch, PCA and StratifiedShufflesplit\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedShuffleSplit\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Validation of the \"Tuned\" Classifiers done via a StratifiedShuffleSplit  \n",
    "ss_split = StratifiedShuffleSplit(labels, 1000, test_size = 0.5, random_state = 42)\n",
    "\n",
    "for train_idx, test_idx in ss_split: \n",
    "        features_train = []\n",
    "        features_test  = []\n",
    "        labels_train   = []\n",
    "        labels_test    = []\n",
    "        for ii in train_idx:\n",
    "            features_train.append( features[ii] )\n",
    "            labels_train.append( labels[ii] )\n",
    "        for jj in test_idx:\n",
    "            features_test.append( features[jj] )\n",
    "            labels_test.append( labels[jj] )    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Validation with Parameter Tuning\n",
      "Best Score: 0.867638888889\n",
      "Best parameters: {'max_features': 3}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=5, max_features=3, max_leaf_nodes=None,\n",
      "            min_samples_leaf=2, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=200, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      "Precision:  1.0\n",
      "Recall:  0.777777777778\n"
     ]
    }
   ],
   "source": [
    "### RandomForest with Parameter Tuning\n",
    "\n",
    "clf = RandomForestClassifier(criterion = 'gini', min_samples_split = 2, max_leaf_nodes = None, max_depth = 5, \n",
    "                             min_samples_leaf = 2, random_state = 0, n_estimators = 200)\n",
    "\n",
    "parameters = {'max_features': [3, 5]}\n",
    "\n",
    "grid_search_rf = GridSearchCV(clf, parameters, cv = ss_split)\n",
    "grid_search_rf.fit(features, labels)\n",
    "\n",
    "clf = grid_search_rf.best_estimator_\n",
    "\n",
    "clf.fit(features, labels)\n",
    "\n",
    "print 'RandomForest Validation with Parameter Tuning'\n",
    "print ('Best Score: {}'.format(grid_search_rf.best_score_))\n",
    "print('Best parameters: {}'.format(grid_search_rf.best_params_))\n",
    "print clf\n",
    "\n",
    "pred_rf= clf.predict(features_test)\n",
    "\n",
    "pre_rf = precision_score(labels_test, pred_rf, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Precision: \",pre_rf\n",
    "rec_rf = recall_score(labels_test, pred_rf, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"Recall: \",rec_rf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy SVC 0.902777777778\n",
      "pre 0.75\n",
      "rec 0.333333333333\n",
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, n_components=5, whiten=False)), ('univ_select', SelectKBest(k=4, score_func=<function f_classif at 0x0000000006C28EB8>))],\n",
      "       transformer_weights=None)), ('SupportVectorClassifier', SVC(C=1000, cache_size=200, class_weight=None, coef0=0.0, degree=3,\n",
      "  gamma=0.001, kernel='linear', max_iter=-1, probability=False,\n",
      "  random_state=42, shrinking=True, tol=0.001, verbose=False))])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Peter\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\classification.py:958: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "### Support Vector Machine with Parameter Tuning\n",
    "\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "skb = SelectKBest(k = 4)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", skb)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "features_x = combined_features.fit(features, labels).transform(features)\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"SupportVectorClassifier\", SVC(random_state=42))])\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "          'SupportVectorClassifier__C': [1, 10, 100, 1000], \n",
    "          'SupportVectorClassifier__gamma': [0.001, 0.0001], \n",
    "          'SupportVectorClassifier__kernel': ['rbf', 'linear'],\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid = param_grid, scoring = 'f1', cv = ss_split)\n",
    "gs.fit(features, labels)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "clf.fit(features, labels)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "accuracy = clf.score(features_test, labels_test)\n",
    "print \"accuracy SVC\",accuracy\n",
    "\n",
    "pre = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"pre\",pre\n",
    "rec = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"rec\",rec\n",
    "\n",
    "print clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy GaussianNB 0.861111111111\n",
      "pre 0.444444444444\n",
      "rec 0.444444444444\n",
      "Pipeline(steps=[('features', FeatureUnion(n_jobs=1,\n",
      "       transformer_list=[('pca', PCA(copy=True, n_components=4, whiten=False)), ('univ_select', SelectKBest(k=4, score_func=<function f_classif at 0x0000000006C28EB8>))],\n",
      "       transformer_weights=None)), ('NaiveBayes', GaussianNB())])\n"
     ]
    }
   ],
   "source": [
    "### GaussianNB with Parameter Tuning\n",
    "\n",
    "clf = GaussianNB()\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "skb = SelectKBest(k = 4)\n",
    "\n",
    "pca = PCA(n_components=5)\n",
    "\n",
    "# Build estimator from PCA and Univariate selection:\n",
    "\n",
    "combined_features = FeatureUnion([(\"pca\", pca), (\"univ_select\", skb)])\n",
    "\n",
    "# Use combined features to transform dataset:\n",
    "features_x = combined_features.fit(features, labels).transform(features)\n",
    "\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"NaiveBayes\", GaussianNB())])\n",
    "\n",
    "pa = dict(features__pca__n_components=[2, 3, 4, 5],\n",
    "                  features__univ_select__k=[1, 2, 3, 4])\n",
    "\n",
    "gs = GridSearchCV(pipeline, param_grid = pa, scoring = 'f1', cv = ss_split)\n",
    "gs.fit(features, labels)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "\n",
    "clf.fit(features, labels)\n",
    "pred = clf.predict(features_test)\n",
    "\n",
    "accuracy = clf.score(features_test, labels_test)\n",
    "print \"accuracy GaussianNB\",accuracy\n",
    "\n",
    "pre = precision_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"pre\",pre\n",
    "rec = recall_score(labels_test, pred, labels=None, pos_label=1, average='binary', sample_weight=None)\n",
    "print \"rec\",rec\n",
    "\n",
    "print clf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    \n",
    "### Task 6: Dump your classifier, dataset, and features_list so anyone can\n",
    "### check your results. You do not need to change anything below, but make sure\n",
    "### that the version of poi_id.py that you submit can be run on its own and\n",
    "### generates the necessary .pkl files for validating your results.\n",
    "\n",
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
